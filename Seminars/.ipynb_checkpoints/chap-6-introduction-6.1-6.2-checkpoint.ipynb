{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Deep Feedforward Network\n",
    "**Other names:**\n",
    "- feedforward neural networks\n",
    "- multilayer perceptrons (MLPs)\n",
    "\n",
    "**Goal:** approximate a function $f^*$\n",
    "\n",
    "[](cần tìm thêm ví dụ)\n",
    "Ex: a classifier $y = f^∗ (x)$ maps an input $\\mathbf{x}$ to a category $\\mathbf{y}$\n",
    "\n",
    "**Deep feedforward networks**: defines a mapping $y = f^*(\\mathbf{x}; \\mathbf{\\theta})$ and learns the values of the parameters $\\mathbf{\\theta}$ -> best function approximation.\n",
    "### Significance\n",
    "- Form basis for many commercial applications\n",
    "    - **Convolutional networks** (specialized feedforward networks): object recognition from photos\n",
    "    - **Recurrent networks**: NLP applications\n",
    "    \n",
    "### Why \"feedforward\" ?\n",
    "- Information flows from input $x$ to output $y$.\n",
    "- No feedback connections from output to itself.\n",
    "\n",
    "-> **Recurrent neural networks**: has feedback connection.\n",
    "\n",
    "### Why \"network\" ?\n",
    "- it is represented by composing together many different functions (connected in a chain)\n",
    "- Each function is called **layer**. Layer consists of many **units** that act in parallel (resembles a neuron).\n",
    "\n",
    "-> The **depth** of the model = the length of the chain\n",
    "-> Each hidden layer of the network is typically vector-valued. The dimensionality of these hidden layers determines the **width** of the model.\n",
    "\n",
    "The learning algorithm must decide how to use those layers to produce the desired output, but the training data does not say what each individual layer should do. -> **hidden layers**\n",
    "\n",
    "Ex: $y = f^3(f^2(f^1(\\mathbf{x})))$. $f^1$ is the *first layer*, $f^3$ is *output layer*.\n",
    "\n",
    "### Why \"neural\" ?\n",
    "- It is loosely inspired by **neuroscience**.\n",
    "\n",
    "-> It is best to think of feedforward networks as function approximation machines that are designed to achieve  statistical generalization, occasionally drawing some insights from what we know about the brain, rather than as models of brain function.\n",
    "\n",
    "## Extending Linear Models\n",
    "To represent nonlinear functions of $x$, we can apply the linear model to a transformed input $φ(x)$, where $φ$ is **nonlinear transformation**\n",
    "[](Equivalently kernel trick of SVM obtains nonlinearity)\n",
    "\n",
    "We can think of $φ$ as providing a set of features describing $x$, or as providing a new representation for $x$.\n",
    "\n",
    "The question is then how to choose the mapping φ. -> The strategy of deep learning is to learn φ\n",
    "\n",
    "## Learn Features\n",
    "In this approach, we have the model: \n",
    "\n",
    "$y = f(\\mathbf{x}; θ, \\mathbf{w}) = φ(\\mathbf{x}; θ)^T \\mathbf{w}$\n",
    "\n",
    "[](learn more about this)\n",
    "- θ used to learn ϕ from broad class of functions\n",
    "- Parameters w map from ϕ (x) to output\n",
    "- Defines FFN where ϕ define a hidden layer \n",
    "\n",
    "\n",
    "[](### Pros and Cons   gives-up on convexity of training, outweigh harms. The advantage is that the human designer only needs to find the right general function family rather than finding precisely the right function.)\n",
    "\n",
    "Feedforward networks have introduced the concept of a hidden layer, and this requires us to choose the **activation functions** that will be used to compute the hidden layer values\n",
    "\n",
    "Learning in deep neural networks requires computing the gradients of complicated functions. We present the **back-propagation** algorithm and its modern generalizations, which can be used to efficiently compute these gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Learning XOR\n",
    "**XOR**: A binary operator on $x_1$ and $x_2$. When exactly one value equals 1, it returns 1, otherwise it returns 0.\n",
    "\n",
    "**Goal**:\n",
    "- Adapt $\\mathbf{\\theta}$ to make our model $y = f(\\mathbf{x}; \\mathbf{\\theta})$ is similar to target fuction $f^*$\n",
    "\n",
    "**Challenge**:\n",
    "- is to fit the training set, which mean performing correctly on four training points: $\\mathbf{x} = \\{[0, 0], [0, 1], [1, 0], [1, 1]\\}$\n",
    "\n",
    "\n",
    "## Linear model ?\n",
    "**MSE loss function**:\n",
    "\n",
    "$\\mathbf{J}(\\mathbf{\\theta}) = \\frac{1}{4} \\sum_{x \\in X} (f^*(x) − f(x; θ))^2$\n",
    "\n",
    "**The form of the model** (linear model): \n",
    "\n",
    "$f(\\mathbf{x}; \\mathbf{w}, b) = \\mathbf{x}^T \\mathbf{w} + b = x_1 w_1 + x_2 w_2 + b$\n",
    "\n",
    "After minimizing the loss function, we will obtain $w = 0$ and $b = 0.5$ -> the model outputs 0.5 everywhere.\n",
    "\n",
    "A linear model **is not able** to represent the XOR function. -> We need a better solution.\n",
    "\n",
    "- use a model to learn a different representation\n",
    "- use a simple feedforward network\n",
    "\n",
    "## Feedforward Network Solution\n",
    "![Simple Feedforward Network](simple-feedforward-network.png)\n",
    "One way to solve this problem is to use a model that learns a different feature space in which a linear model is able to represent the solution.\n",
    "\n",
    "### The complete model:\n",
    "$y = f(\\mathbf{x}; \\mathbf{W}, c, w, b) = f^{(2)}(f^{(1)})(\\mathbf{x}))$\n",
    "\n",
    "**Layer 1 (hidden layer):**\n",
    "\n",
    "$\\mathbf{h} = f^{(1)}(\\mathbf{x}; \\mathbf{W} , \\mathbf{c})$ \n",
    "\n",
    "**Layer 2 (output layer):**\n",
    "\n",
    "$y = f^{(2)}(\\mathbf{h}; \\mathbf{w}, b)$\n",
    "\n",
    "We need a nonlinear activation function to describe the features. ->  \n",
    "$\\mathbf{h} = g(\\mathbf{W}^{T} \\mathbf{x} + \\mathbf{c})$\n",
    "\n",
    "![Learned h space](h-space.png)\n",
    "\n",
    "### Default Activation Function\n",
    "![ReLU](relu.png)\n",
    "The recommendation is to use the **rectified linear unit** or ReLU. \n",
    "\n",
    "$g(z) = max\\{0, z\\}$\n",
    "\n",
    "Because rectified linear units are nearly linear:\n",
    "- they preserve many of the properties that make linear models easy to optimize with gradientbased methods. \n",
    "- They also preserve many of the properties that make linear models generalize well. \n",
    "\n",
    "#### The complete model:\n",
    "$f(\\mathbf{x}; \\mathbf{W}, \\mathbf{c}, \\mathbf{w}, b) = \\mathbf{w}^T max\\{0, \\mathbf{W}^T \\mathbf{x}\\ + \\mathbf{c}\\} + b$\n",
    "\n",
    "## Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# import the numpy library\n",
    "import numpy as np\n",
    "\n",
    "# input\n",
    "X = np.matrix([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "# function\n",
    "def xor(input):\n",
    "    # parameters\n",
    "    W = np.matrix([[1,1],[1,1]])\n",
    "    c = np.matrix([0,-1])\n",
    "    w = np.matrix([1,-2]).T\n",
    "    b = 0\n",
    "    \n",
    "    # rectified linear unit\n",
    "    h = np.maximum(X * W + c, 0)\n",
    "    \n",
    "    output = h * w + b\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(xor(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real situation, there might be billions of model parameters and billions of training examples, so one cannot simply guess the solution as we did here. Instead, a **gradient-based optimization** algorithm can find parameters that produce very little error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
