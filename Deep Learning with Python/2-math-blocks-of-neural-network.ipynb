{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first look at a neural network\n",
    "Let’s look at a concrete example of a neural network that uses the Python library **Keras** to learn to classify handwritten digits. \n",
    "\n",
    "You can think of “solving” MNIST as the “Hello World” of deep learning.\n",
    "\n",
    "### Import the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhdq99hp/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "The core building block of neural networks is the layer. Specifically, layers extract representations out of the data fed into them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the network ready for training, we need to pick three more things, as part of the compilation step:\n",
    "- A **loss function**—How the network will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction.\n",
    "- An **optimizer**—The mechanism through which the network will update itself based on the data it sees and its loss function.\n",
    "- **Metrics** to monitor during training and testing—Here, we’ll only care about accuracy (the fraction of the images that were correctly classified)\n",
    "\n",
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.2600 - acc: 0.9251\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.1038 - acc: 0.9686\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0682 - acc: 0.9797\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0493 - acc: 0.9849\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0366 - acc: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89593187b8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 51us/step\n",
      "Test accuracy: 97.68 %\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('Test accuracy: ' + str(test_acc * 100) + ' %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data representations for neural networks\n",
    "### Scalars\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,5,1,3,6])\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vector has five entries and so is called a *5-dimensional vector*. Don’t confuse a 5D vector with a 5D tensor! \n",
    "\n",
    "A 5D vector has only one axis and has five dimensions along its axis, whereas a 5D tensor has five axes (and may have any number of dimensions along each axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 78,  2, 34,  0],\n",
       "       [ 6, 79,  3, 35,  1],\n",
       "       [ 7, 80,  4, 36,  2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[5, 78, 2, 34, 0],\n",
    "[6, 79, 3, 35, 1],\n",
    "[7, 80, 4, 36, 2]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D tensors and higher-dimensional tensors\n",
    "In deep learning, you’ll generally manipulate tensors that are 0D to 4D, although you may go up to\n",
    "5D if you process video data.\n",
    "\n",
    "### Key attributes\n",
    "- Number of axes (rank)\n",
    "- Shape\n",
    "- Data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "print(train_images.ndim)\n",
    "print(train_images.shape)\n",
    "print(train_images.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1391b5978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = train_images[10]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating tensors in Numpy\n",
    "#### tensor slicing\n",
    "\n",
    "#### What is batch ?\n",
    "In general, the first axis (axis 0, because indexing starts at 0) in all data tensors you’ll come across in deep learning will be the *samples axis*.\n",
    "\n",
    "Deep-learning models don’t process an entire dataset at once; rather, they break the data into small batches\n",
    "\n",
    "### Real-world examples of data tensors\n",
    "- Vector data—2D tensors of shape (`samples, features`)\n",
    "- Timeseries data or sequence data—3D tensors of shape (`samples, timesteps, features`)\n",
    "- Images—4D tensors of shape (`samples, height, width, channels`) or (`samples, channels, height, width`)\n",
    "- Video—5D tensors of shape (`samples, frames, height, width, channels`) or (`samples, frames, channels, height, width`)\n",
    "\n",
    "## The gears of neural networks: tensor operations\n",
    "Ex: `keras.layers.Dense(512, activation='relu')` can be interpreted as a function which takes as input a 2D tensor and returns another 2D tensor—*a new representation* for the input tensor.\n",
    "`output = relu(dot(W, input) + b)`\n",
    "\n",
    "### Element-wise operations\n",
    "The `relu` operation and `addition` are element-wise operations: operations that are\n",
    "applied independently to each entry in the tensors being considered.\n",
    "\n",
    "In `numpy`:\n",
    "- Element-wise adition: `x = a + b`\n",
    "- Element-wise relu: `x = np.maximum(0, z)`\n",
    "\n",
    "### Broadcasting\n",
    "we added a 2D tensor with a vector.\n",
    "\n",
    "### Tensor dot\n",
    "The dot operation, also called a *tensor product* (not to be confused with an elementwise product)\n",
    "\n",
    "You can also take the dot product between a matrix x and a vector y, which returns a vector where the coefficients are the dot products between y and the rows of x\n",
    "\n",
    "### Tensor reshaping\n",
    "A special case of reshaping that’s commonly encountered is transposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 20)\n",
      "(20, 30)\n"
     ]
    }
   ],
   "source": [
    "x = np.ones((30,20))\n",
    "print(x.shape)\n",
    "x = np.transpose(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The engine of neural networks: gradient-based optimization\n",
    "Repeat these steps in a loop, as long as necessary:\n",
    "1. Draw a batch of training samples x and corresponding targets y.\n",
    "2. Run the network on x (a step called the forward pass) to obtain predictions y_pred.\n",
    "3. Compute the loss of the network on the batch, a measure of the mismatch between y_pred and y.\n",
    "4. Compute the gradient of the loss with regard to the network’s parameters (a backward pass).\n",
    "5. Move the parameters a little in the opposite direction from the gradient—for example `W -= step * gradient` —thus reducing the loss on the batch a bit.\n",
    "\n",
    "### Stochastic gradient descent\n",
    "There exist multiple variants of SGD that differ by taking into account previous weight updates when computing the next weight update, rather than just looking at the current value of the gradients.\n",
    "\n",
    "In particular, the concept of momentum, which is used in many of these variants, deserves your attention. Momentum addresses two issues with SGD: \n",
    "- Convergence speed \n",
    "- Local minima\n",
    "\n",
    "### Chaining derivatives: the Backpropagation algorithm\n",
    "Calculus tells us that such a chain of functions can be derived using the following identity, called the chain rule: $f(g(x)) = f'(g(x)) * g'(x)$\n",
    "\n",
    "Applying the chain rule to the computation of the gradient values of a neural network gives rise to an algorithm called **Backpropagation**\n",
    "\n",
    "Nowadays, and for years to come, people will implement networks in modern frameworks that are capable of **symbolic differentiation**, such as TensorFlow\n",
    "\n",
    "Thanks to symbolic differentiation, you’ll never have to implement the Backpropagation algorithm by hand"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
