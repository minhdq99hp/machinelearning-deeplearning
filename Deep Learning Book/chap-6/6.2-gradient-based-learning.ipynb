{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Designing and training a neural network is not much different from training any\n",
    "other ML model with gradient descent. It needs:\n",
    "- optimization procedure,\n",
    "- a cost function\n",
    "- a model family.\n",
    "\n",
    "The nonlinearity of a neural network causes most interesting loss functions to become non-convex.\n",
    "\n",
    "-> Neural networks are usually trained by using **iterative, gradient-based optimizers** rather than linear equation solver or convex optimization algorithms or SVMs.\n",
    "\n",
    "Stochastic gradient descent applied to non-convex loss functions has no global convergence guarantee, and is sensitive to the values of the initial parameters. \n",
    "\n",
    "![non-convex vs convex][convex_cost_function]\n",
    "\n",
    "-> For feedforward neural networks, it is important to: \n",
    "- initialize all weights to small random values. \n",
    "- initialize the biases to zero or to small positive values.\n",
    "\n",
    "Computing the gradient is slightly more complicated for a neural network, but can still be done efficiently and exactly.\n",
    "\n",
    "## Cost function\n",
    "The cost functions for neural networks are more or less the same as those for other parametric models\n",
    "\n",
    "[](What is this ?)\n",
    "In most cases, our parametric model defines a distribution $p(\\mathbf{y}\\ |\\ \\mathbf{x};\\mathbf{\\theta})$ and we simply use the principle of **maximum likelihood**. \n",
    "\n",
    "-> we use the **cross-entropy** between the training data and the model’s predictions as the cost function.\n",
    "\n",
    "### Learning Conditional Distributions with Maximum Likelihood\n",
    "Most modern neural networks are trained using maximum likelihood.\n",
    "\n",
    "-> the cost function is simply the **negative log-likelihood** or equivalently descrived as the cross-entropy between the training data and the model distribution.\n",
    "\n",
    "$\\mathbf{J}(\\theta) = −E_{x,y∼\\hat{p}_{data}} log p_{model}(\\mathbf{y} | \\mathbf{x})$\n",
    "\n",
    "The specific form of the cost function changes from model to model, depending on the specific form of log pmodel. The expansion of the above equation typically yields some terms that do not depend on the model parameters and may be discarded.\n",
    "\n",
    "An advantage of this approach of deriving the cost function from maximum likelihood is that it removes the burden of designing cost functions for each model.\n",
    "\n",
    "-> Specifying a model $p(\\mathbf{y} | \\mathbf{x})$ automatically determines a cost function $log p(\\mathbf{y} | \\mathbf{x})$\n",
    "\n",
    "One recurring theme throughout neural network design is that the gradient of the cost function must be large and predictable enough to serve as a good guide for the learning algorithm.\n",
    "\n",
    "Negative log-likelihood avoids saturation problems\n",
    "\n",
    "### Learning Conditional Statistics\n",
    "Instead of learning a full probability distribution $p(\\mathbf{y} | \\mathbf{x}; \\mathbf{\\theta})$ we often want to learn just one conditional statistic of $\\mathbf{y}$ given $\\mathbf{x}$.\n",
    "\n",
    "Ex: we may have a predictor $f(\\mathbf{x}; \\mathbf{\\theta})$ that we wish to predict the mean\n",
    "of $\\mathbf{y}$\n",
    "\n",
    "We can view the cost function as being a **functional** rather than just a function. A functional is a mapping from functions to real numbers. We can thus think of learning as choosing a function rather than merely choosing a set of parameters.\n",
    "\n",
    "Solving an optimization problem with respect to a function requires a mathematical tool called **calculus of variations**\n",
    "\n",
    "Different cost functions give different statistics\n",
    "-> We can predicts the median or the mean value of $\\mathbf{y}$ for each $\\mathbf{x}$\n",
    "\n",
    "#### Conclusion\n",
    "**Mean squared error** and **mean absolute error** often lead to poor results when used with gradient-based optimization.\n",
    "\n",
    "-> The cross-entropy cost function is more popular, even when it is not necessary to estimate an\n",
    "entire distribution $p(\\mathbf{y} | \\mathbf{x})$.\n",
    "\n",
    "[](Phần này chưa hiểu rõ lắm)\n",
    "## Output units\n",
    "Most of the time, we simply use the cross-entropy between the data distribution and the model distribution. The choice of how to represent the output then determines the form of the cross-entropy function.\n",
    "\n",
    "### Linear Units for Gaussian Output Distributions\n",
    "output unit based on an affine transformation with no nonlinearity.\n",
    "\n",
    "Given features $\\mathbf{h}$, a layer of linear output units produces a vector $\\hat{\\mathbf{y}} = \\mathbf{W}^T \\mathbf{h}+\\mathbf{b}$\n",
    "\n",
    "Linear output layers are often used to produce the mean of a conditional Gaussian distribution.\n",
    "\n",
    "linear units do not saturate, they pose little difficulty for gradientbased optimization algorithms and may be used with a wide variety of optimization algorithms.\n",
    "\n",
    "### Sigmoid Units for Bernoulli Output Distributions\n",
    "-> predicting the value of a binary variable y\n",
    "\n",
    "A Bernoulli distribution is defined by just a single number. The neural net needs to predict only $P(\\mathbf{y} = 1 | \\mathbf{x})$. For this number to be a valid probability, it must lie in the interval $[0, 1]$\n",
    "\n",
    "A sigmoid output unit is defined by\n",
    "\n",
    "$\\hat{\\mathbf{y}} = \\sigma (\\mathbf{W}^T \\mathbf{h} + \\mathbf{b})$\n",
    "\n",
    "We can think of the sigmoid output unit as having two components. \n",
    "- First, it uses a linear layer to compute $\\mathbf{z} = \\mathbf{w}^T \\mathbf{h} + \\mathbf{b}$. \n",
    "- Next, it uses the sigmoid activation function to convert $\\mathbf{z}$ into a probability.\n",
    "\n",
    "### Softmax Units for Multinoulli Output Distributions\n",
    "Any time we wish to represent a probability distribution over a discrete variable with n possible values, we may use the softmax function. This can be seen as a generalization of the sigmoid function which was used to represent a probability distribution over a binary variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[convex_cost_function]: convex_cost_function.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
