{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Rectified linear units are an excellent default choice of hidden unit.\n",
    "\n",
    "There are many other types of hidden units -> need trial and error to find the one that work best.\n",
    "\n",
    "Some of the hidden units included in this list are not actually differentiable at all input points.\n",
    "\n",
    "## What a hidden unit does ?\n",
    "1. Accepting a vector of inputs $\\mathbf{x}$, computing an affine transformation $\\mathbf{z} = \\mathbf{W}^T\\mathbf{x} + \\mathbf{b}$ \n",
    "2. Applying an element-wise nonlinear function $g(\\mathbf{z})$\n",
    "\n",
    "-> Most hidden units are distinguished from each other only by the choice of the form of the activation\n",
    "function $g(\\mathbf{z})$\n",
    "\n",
    "## Rectified Linear Units and Their Generalizations\n",
    "$g(z) = max\\{0, z\\}$\n",
    "\n",
    "They are easy to optimize due to similarity with linear units\n",
    "- Only difference with linear units that they output 0 across half its domain\n",
    "- Derivative is 1 everywhere that the unit is active\n",
    "- Thus gradient direction is far more useful than with activation functions with second-order effects\n",
    "\n",
    "Usually used on top of an affine transformation:\n",
    "\n",
    "$\\mathbf{h}=g(\\mathbf{W}^T \\mathbf{x}+\\mathbf{b})$\n",
    "\n",
    "It is good to set all elements of b to a small value such as 0.1\n",
    "-> This makes it likely that ReLU will be initially active for most training samples and allow derivatives to\n",
    "pass through.\n",
    "\n",
    "### Generalizations of ReLU\n",
    "- Perform comparably to ReLU and occasionally perform better\n",
    "- ReLU cannot learn on examples for which the activation is zero.\n",
    "- Generalizations guarantee that they receive gradient everywhere\n",
    "\n",
    "Three generalizations of rectified linear units are based on using a non-zero slope $α_i$ when $z_i < 0$: \n",
    "\n",
    "$h_i = g(z, α)_i = max(0, z_i) + α_i min(0, z_i)$\n",
    "\n",
    "1. Absolute-value rectification:\n",
    "    - fixes $α_i =-1$ to obtain $g(z)=|z|$\n",
    "2. Leaky ReLU:\n",
    "    - fixes $α_i$ to a small value like $0.01$\n",
    "3. Parametric ReLU or PReLU:\n",
    "    - treats $α_i$ as a parameter\n",
    "\n",
    "[](Chưa hiểu phần này lắm)\n",
    "#### Maxout Units\n",
    "\n",
    "## Logistic Sigmoid and Hyperbolic Tangent\n",
    "$g(z) = \\sigma(z)$\n",
    "\n",
    "or\n",
    "\n",
    "$g(z) = \\tanh(z)$\n",
    "\n",
    "These activation functions are closely related because: $\\tanh(z)=2\\sigma(2z)-1$\n",
    "\n",
    "Sigmoid units are used to predict probability that a binary variable is 1\n",
    "\n",
    "## Other hidden units\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
