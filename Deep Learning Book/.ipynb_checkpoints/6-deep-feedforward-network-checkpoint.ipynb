{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept\n",
    "**Deep feedforward networks**,(feedforward neural networks),\n",
    "or multilayer perceptrons (MLPs)\n",
    "\n",
    "Goal: approximate a function $f^*$\n",
    "\n",
    "### Why \"feedforward\"\n",
    "Because information flows through the function being evaluated from x, through the intermediate computations used to define f, and finally to the output y.\n",
    "\n",
    "There are no feedback connections in which outputs of the model are fed back into itself.\n",
    "**Recurrent neural networks** has feedback connection from output to input.\n",
    "\n",
    "### Why \"network\"\n",
    "Because it is represented by composing together many different functions\n",
    "\n",
    "Each function is called **layer**. Layer consists of many **units** that act in parallel (resembles a neuron).\n",
    "\n",
    "The **depth** of the model = the length of the chain\n",
    "Each hidden layer of the network is typically vector-valued. The dimensionality of these hidden layers determines the **width** of the model.\n",
    "\n",
    "The learning algorithm must decide how to use those layers to produce the desired output, but the training data does not say what each individual layer should do. -> **hidden layers**\n",
    "\n",
    "### Why \"neural\"\n",
    "Because it is loosely inspired by **neuroscience**.\n",
    "\n",
    "To extend linear models to represent nonlinear functions of $x$, we can apply the linear model not to $x$ itself but to a transformed input $φ(x)$, where $φ$ is **nonlinear transformation** Ex: kernel trick\n",
    "\n",
    "The question is then how to choose the mapping φ. The strategy of deep learning is to learn φ\n",
    "\n",
    "In this approach, we have the model: $y = f(\\mathbf{x}; θ, \\mathbf{w}) = φ(\\mathbf{x}; θ)^T \\mathbf{w}$\n",
    "\n",
    "Feedforward networks have introduced the concept of a hidden layer, and this requires us to choose the **activation functions** that will be used to compute the hidden layer values\n",
    "\n",
    "Learning in deep neural networks requires computing the gradients of complicated functions. We present the **back-propagation** algorithm and its modern generalizations, which can be used to efficiently compute these gradients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
