{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Designing and training a neural network is not much different from training any\n",
    "other ML model with gradient descent. It needs:\n",
    "- optimization procedure,\n",
    "- a cost function\n",
    "- a model family.\n",
    "\n",
    "The nonlinearity of a neural network causes most interesting loss functions to become non-convex.\n",
    "\n",
    "-> Neural networks are usually trained by using **iterative, gradient-based optimizers** rather than linear equation solver or convex optimization algorithms or SVMs.\n",
    "\n",
    "Stochastic gradient descent applied to non-convex loss functions has no global convergence guarantee, and is sensitive to the values of the initial parameters. \n",
    "\n",
    "![non-convex vs convex][convex_cost_function]\n",
    "\n",
    "-> For feedforward neural networks, it is important to: \n",
    "- initialize all weights to small random values. \n",
    "- initialize the biases to zero or to small positive values.\n",
    "\n",
    "Computing the gradient is slightly more complicated for a neural network, but can still be done efficiently and exactly.\n",
    "\n",
    "## Cost function\n",
    "The cost functions for neural networks are more or less the same as those for other parametric models\n",
    "\n",
    "[](What is this ?)\n",
    "In most cases, our parametric model defines a distribution $p(\\mathbf{y}\\ |\\ \\mathbf{x};\\mathbf{\\theta})$ and we simply use the principle of **maximum likelihood**. \n",
    "\n",
    "-> we use the **cross-entropy** between the training data and the model’s predictions as the cost function.\n",
    "\n",
    "### Learning Conditional Distributions with Maximum Likelihood\n",
    "Most modern neural networks are trained using maximum likelihood.\n",
    "\n",
    "-> the cost function is simply the **negative log-likelihood** or equivalently descrived as the cross-entropy between the training data and the model distribution.\n",
    "\n",
    "$\\mathbf{J}(\\theta) = −E_{x,y∼\\hat{p}_{data}} log p_{model}(\\mathbf{y} | \\mathbf{x})$\n",
    "\n",
    "The specific form of the cost function changes from model to model, depending on the specific form of log pmodel. The expansion of the above equation typically yields some terms that do not depend on the model parameters and may be discarded.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[convex_cost_function]: convex_cost_function.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
