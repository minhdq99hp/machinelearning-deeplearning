{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 6: Deep Feedforward Networks\n",
    "\n",
    "## Introduction\n",
    "- Names: Deep feedforward networks/feedforward neural networks/multiplayer perceptron (MLPs)\n",
    "- Goal: approximate some function $f^*$\n",
    "- Why \"feedforward\", \"neural\", \"network\" ?\n",
    "- It is composed of many different functions. (chain function). -> layers -> units\n",
    "- Depth of the model = the length of the chain function\n",
    "-> Hidden layers -> the dimensionality of these hidden layers determines the width of the model.\n",
    "- Significance: -> convolution networks, recurrent networks\n",
    "\n",
    "### Extending linear models\n",
    "To extend linear models to represent nonlinear functions of x -> apply the linear model to transformed input $\\phi x$ ($\\phi$ is a nonlinear transformation)\n",
    "\n",
    "-> The strategy of deep learning is to learn $\\phi$ from a board class of functions by parameters $\\theta$\n",
    "\n",
    "## 6.1 Example: Learning XOR\n",
    "- A linear model can't learn XOR function. -> use deep feedforward network\n",
    "-> introduce to activation function ReLU\n",
    "\n",
    "- Complete model: $f(\\mathbf{x}; \\mathbf{W}, \\mathbf{c}, \\mathbf{w}, b) = \\mathbf{w}^T max\\{0, \\mathbf{W}^T \\mathbf{x}\\ + \\mathbf{c}\\} + b$\n",
    "\n",
    "-> a gradient-based optimization algorithm can find parameters that produce very little error.\n",
    "\n",
    "## 6.2 Gradient-Based Learning\n",
    "- Designing and training a neural network needs:\n",
    "    optimization procedure,\n",
    "    a cost function\n",
    "    a model family.\n",
    "    \n",
    "- The nonlinearity of a neural network causes loss functions to become **non-convex**.\n",
    "![][convex_cost_function]\n",
    "-> use **iterative, gradient-based optimizers** rather than linear equation solver or convex optimization algorithms or SVMs\n",
    "\n",
    "### Cost functions\n",
    "- We use the cross-entropy between the training data and the model's predictions as the cost function.\n",
    "\n",
    "#### Learning Conditional Distributions with Maximum Likelihood\n",
    "- Modern NN use **maximum likelihood** -> the cost function is simply the **negative log-likelihood**.\n",
    "- $\\mathbf{J}(\\theta) = −E_{x,y∼\\hat{p}_{data}} log p_{model}(\\mathbf{y} | \\mathbf{x})$\n",
    "- The specific form of the cost functions changes from model to model, depending on the $log p_{model}$ \n",
    "- advantages: \n",
    "    - remove the burden of designing cost functions.\n",
    "    - avoid saturation problems.\n",
    "\n",
    "#### Learning Conditional Statistics\n",
    "- If we use a sufficiently powerful NN,... -> think learning as choosing function rather than choosing parameters. -> to optimize problem wrt a fuction: **calculus of variations**\n",
    "- So it can give us 2 following results:\n",
    "    - predict the **mean** of y for each value of x.\n",
    "    - predict the **median**...\n",
    "\n",
    "- Poor results-> the cross-entropy cost function is more popular than mean squared error or mean absolute error.\n",
    "\n",
    "### Output Units\n",
    "- Output units can be used as hidden units.\n",
    "- Role: additional transformation\n",
    "\n",
    "#### Linear Units for Gaussian Output Distributions\n",
    "- In: $\\mathbf{h}$ -> Out: $\\hat{y} = \\mathbf{W}^T \\mathbf{h} + \\mathbf{b}$.\n",
    "- often used to produce the mean of a conditional Gauss dist.\n",
    "- linear units don't sat -> litte difficult for gradient-based algo.\n",
    "\n",
    "#### Sigmoid Units for Bernoulli Output Distributions\n",
    "Tasks require predicting the value of a binary variable y.\n",
    "\n",
    "Out: $\\hat{y} = \\sigma(\\mathbf{w}^T \\mathbf{h} + b)$\n",
    "- first, uses a linear layer to compute $z = w^T h + b$\n",
    "- second, uses sigmoid to convert z into probability.\n",
    "![][sigmoid-unit]\n",
    "\n",
    "#### Softmax Units for Multinoulli Output Distributions\n",
    "- Generalization of the sigmoid function -> probability dist over a discrete variable with $n$ possible values.\n",
    "\n",
    "$z = \\mathbf{W}^T \\mathbf{h} + \\mathbf{b}$\n",
    "\n",
    "$softmax(z)_i = \\frac{exp(z_i)}{\\sum_j exp(z_j)}$\n",
    "\n",
    "## 6.3 Hidden Units\n",
    "- Good default choice: **ReLU**\n",
    "- There are more but need trial and error to choose which one is the best.\n",
    "- left derivative, right derivative are difined and equal -> dfferentiable at z.\n",
    "\n",
    "### ReLU and their Generalizations\n",
    "$g(z) = max\\{0, z\\}$\n",
    "\n",
    "$\\mathbf{h} = g(\\mathbT \\mathbf{x} + \\mathbf{b})$\n",
    "\n",
    "$\\mathbf{b}$ should be small, such as $0.1$\n",
    "- Most generalizations: \n",
    "    - perform better.\n",
    "    - guarantee receiving gradient everywhere.\n",
    "- Ex: leaky ReLU, perametric ReLU, PReLU.\n",
    "\n",
    "#### Maxout units\n",
    "- divide $z$ into groups of $k$ values.\n",
    "![][maxout-unit]\n",
    "\n",
    "### Logistic Sigmoid and Hyperbolic Tangent\n",
    "- Logistic sigmoid: $g(z) = \\sigma(z)$\n",
    "- Hyperbolic tangent: $g(z) = \\tanh(z)$\n",
    "- Their use as hidden units in feedforward networks is now discouraged.\n",
    "- Sigmoidal activation functions are common in settings other than feedforward networks.\n",
    "\n",
    "## 6.4 Architecture Design\n",
    "Architecture: \n",
    "- how many units it should have \n",
    "- how these units should be connected to each other\n",
    "\n",
    "![][nn]\n",
    "Each layer being a function of the layer that preceded it.\n",
    "\n",
    "- main architectural considerations: to choose the **depth** of the network and the **width** of each layer\n",
    "\n",
    "Deeper -> fewer units per layer and parameters\n",
    "\n",
    "### Universal Approximation Properties and Depth\n",
    "**universal approximation theorem** -> any continuous function on a closed and bounded subset of $R^n$ is Borel measurable -> be approximated by a neural network (able to represent but may not able to learn)\n",
    "\n",
    "-> use deeper model to reduct: number of units and the amount of generalization error.\n",
    "\n",
    "### Other Architectural Considerations\n",
    "- how to connect a pair of layers to each other.\n",
    "Ex: convolutional networks for computer vision, recurrent neural networks for sequence processing.\n",
    "\n",
    "These strategies for reducing the number of connections: \n",
    "- reduce the number of parameters and the amount of computation required to evaluate the network \n",
    "- often highly problem-dependent\n",
    "\n",
    "## 6.5 Back-Propagation and Other Differentiation Algorithms\n",
    "**Back-propagation** algo: allows the information from the cost to flow backwards -> compute the gradient.\n",
    "\n",
    "### Computational Graphs\n",
    "-> To describe the back-propagation algorithm more precisely.\n",
    "\n",
    "An **operation** is a simple function of one or more variables. \n",
    "\n",
    "Our graph language is accompanied by a set of allowable operations.\n",
    "\n",
    "![][computational-graph]\n",
    "\n",
    "### Chain Rule of Calculus\n",
    "![][chain-rule]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[convex_cost_function]: convex_cost_function.jpg\n",
    "[sigmoid-unit]: sigmoid-unit.png\n",
    "[maxout-unit]: maxout.jpg\n",
    "[nn]: nn.png\n",
    "[computational-graph]: computational-graph.png\n",
    "[chain-rule]: chain-rule.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
