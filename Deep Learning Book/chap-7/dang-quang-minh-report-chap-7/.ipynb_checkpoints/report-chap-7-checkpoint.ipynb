{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 7: Regularization\n",
    "\n",
    "## Introduction\n",
    "**A central problem in ML**: how to make an algo will perform well on both training data and new inputs.\n",
    "\n",
    "**Regularization**: modification to reduce generalization error but not training error.\n",
    "\n",
    "Adding **constraints** and **penalites** can lead to improved performance on the test set.\n",
    "\n",
    "In the context of deep learning, most regularization strategies are based on **regularizing estimators** (trading increased bias for reduced variance - avoid overfitting)\n",
    "\n",
    "The best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately.\n",
    "\n",
    "## Parameter Norm Penalties\n",
    "Many regularization approaches are based on **limiting the capacity** of models by adding a parameter norm penalty $\\Omega(\\theta)$ to the objective function $J$. \n",
    "\n",
    "$\\tilde{J}(\\theta; X, y) = J(\\theta; X, y) + \\alpha \\Omega(\\theta)$\n",
    "\n",
    "where $\\alpha \\in [0,\\infty)$, $\\alpha \\uparrow \\rightarrow\n",
    "\\text{more generalization}$\n",
    "\n",
    "For neural network, we only choose $\\Omega$ that penalizes only the weights and leaves the biases unregularized.\n",
    "\n",
    "### $L^2$ Parameter Regularization (weight decay)\n",
    "or **ridge regression** or **Tikhonov regularization**\n",
    "\n",
    "$\\Omega(\\theta) = \\frac{1}{2} ||w||^2_2$\n",
    "\n",
    "The addition of the weight decay term has modified the learning rule to multiplicatively shrink the weight vector by a constant factor (slightly less than 1) on each step, just before performing the usual gradient update.\n",
    "\n",
    "-> This prevents the weights from growing too large (which is a problem that cause overfitting)\n",
    "\n",
    "### $L^1$ Regularization\n",
    "$\\Omega(\\theta) = ||w||_1 = \\sum_i |w_i|$\n",
    "\n",
    "In comparison to $L_2$ regularization, $L_1$ regularization results in a solution that is more sparse.\n",
    "\n",
    "The $L_1$ penalty causes a subset of the weights to become zero, suggesting that the corresponding features may safely be discarded.\n",
    "\n",
    "## Norm Penalties as Constrained Optimization\n",
    "we can minimize a function subject to constraints by constructing a generalized Lagrange function.\n",
    "\n",
    "If we wanted to constrain $\\Omega(\\theta)$ to be less than some constant $k$, we could construct a generalized Lagrange function:\n",
    "\n",
    "$L(\\theta, \\alpha; X, y) = J(\\theta; X, y) + \\alpha(\\Omega(\\theta) − k)$\n",
    "\n",
    "The solution to the constrained problem is given by\n",
    "$\\theta^* = argmin_\\theta \\max_{\\alpha \\alpha \\geq 0} L(\\theta, \\alpha)$\n",
    "\n",
    "## Regularization and Under-Constrained Problems\n",
    "Many linear models in machine learning, including linear regression and PCA, depend on inverting the matrix $X^T X$. This is not possible whenever $X^TX$ is singular. \n",
    "\n",
    "-> many forms of regularization correspond to inverting\n",
    "$X^T X + \\alpha I$ -> This regularized matrix is guaranteed to be invertible.\n",
    "\n",
    "## Dataset Augmentation\n",
    "The best way to make a machine learning model generalize better is to train it on more data. -> create fake data and add it to the training set.\n",
    "\n",
    "**Dataset augmentation** has been a particularly effective technique for a specific classification problem: object recognition.\n",
    "\n",
    "Operations like translating, rotating or scaling but not horizontal flips or 180 degree rotations can improve generalization.\n",
    "\n",
    "**Injecting noise** in the input to a neural network can also be seen as a form of data augmentation.\n",
    "\n",
    "Neural networks prove not to be very robust to noise. \n",
    "\n",
    "One way to improve the robustness of neural networks is simply to train them with random noise applied to their\n",
    "inputs\n",
    "\n",
    "## Noise Robustness\n",
    "For some models, the **addition of noise** with infinitesimal variance at the input of the model is equivalent to **imposing a penalty** on the norm of the weights.\n",
    "\n",
    "### Injecting Noise at the Output Targets\n",
    "Most datasets have some amount of mistakes in the $y$ labels. It can be harmful to maximize $log p(y | x)$ when $y$ is a mistake. \n",
    "\n",
    "-> One way to prevent this is to explicitly model the noise on the labels.\n",
    "\n",
    "## Semi-Supervised Learning\n",
    "both unlabeled examples from $P(x)$ and labeled examples from $P (x, y)$ are used to estimate $P (y | x)$ or predict $y$ from $x$.\n",
    "\n",
    "## Multi-Task Learning\n",
    "pooling the examples (which can be seen as soft constraints imposed on the parameters) arising out of several tasks.\n",
    "\n",
    "## Early Stopping\n",
    "When training large models with sufficient representational capacity to overfit the task, we often observe that training error decreases steadily over time, but validation set error begins to rise again.\n",
    "![early-stopping][early-stopping]\n",
    "\n",
    "-> the most commonly used form of regularization in DL because of it effectiveness and its simplicity.\n",
    "\n",
    "\n",
    "The idea behind early stopping is relatively simple:\n",
    "- Split data into training and test sets\n",
    "- At the end of each epoch (or, every N epochs):\n",
    "    - evaluate the network performance on the test set\n",
    "    - if the network outperforms the previous best model: save a copy of the network at the current epoch\n",
    "- Take as our final model the model that has the best test set performance\n",
    "\n",
    "## Parameter Tying and Parameter Sharing\n",
    "Sometimes we might not know precisely what values the parameters should take but we know, from knowledge of the domain and model architecture, that there should be some dependencies between the model parameters.\n",
    "\n",
    "### Parameter Tying\n",
    "We often want to express is that certain parameters should be close to one another. (by using parameter norm penalty of the form: $\\Omega(w_A, w_B) = ||w_A - w_B||^2_2$\n",
    "\n",
    "### Parameter Sharing\n",
    "force sets of parameters to be equal.\n",
    "\n",
    "-> only a subset of the parameters need to be stored in memory.-> reduction in the memory footprint of the model\n",
    "\n",
    "-> very useful in Convolutional Neural Networks: \n",
    "     - allowed CNNs to dramatically lower the number of unique model parameters\n",
    "     - significantly increase network sizes without requiring a corresponding increase in training data.\n",
    "\n",
    "## Sparse Representations\n",
    "Place a penalty on the activations of the units in a neural network -> cause the activations to be sparse\n",
    "\n",
    "Representational sparsity, on the other hand, describes a\n",
    "representation where many of the elements of the  representation are zero (or close to zero)\n",
    "![representational-sparsity][representational-sparsity]\n",
    "\n",
    "Representational regularization is accomplished by the same sorts of mechanisms that we have used in parameter regularization. -> using a norm penalty on the representation.\n",
    "\n",
    "L1 penalty on the elements of the representation induces representational sparsity\n",
    "\n",
    "Other approaches obtain representational sparsity with a hard constrain on the activation values: Ex: orthogonal matching pursuit.\n",
    "\n",
    "## Bagging and Other Ensemble Methods\n",
    "**Bagging** is a technique for reducing generalization error by combining serveral models.\n",
    "\n",
    "The reason that **model averaging** works is that different models will usually not make all the same errors on the test set.\n",
    "\n",
    "Bagging is a method that allows the same kind of model, training algorithm and objective function to be reused several times\n",
    "\n",
    "Ex: ![bagging][bagging]\n",
    "\n",
    "**Boosting** constructs an ensemble with higher capacity then the individual models.\n",
    "\n",
    "## Dropout\n",
    "provides a computationally inexpensive but powerful method of regularizing a broad family of models\n",
    "\n",
    "dropout: making bagging practical for ensembles of very many large neural networks.\n",
    "\n",
    "dropout trains the ensemble consisting of all sub-networks that can be formed by **removing non-output units** (by multiplying it with 0) from an underlying base network.\n",
    "\n",
    "\n",
    "In the case of dropout:\n",
    "- the models **share parameters**, with each model inheriting a different subset of parameters from the parent neural network.\n",
    "- typically most models are not explicitly trained at all—usually\n",
    "- The prediction of the ensemble: $\\sum_\\mu p(\\mu) p(y|x,\\mu)$\n",
    "\n",
    "**Advantage**: \n",
    "- computationally cheap. Using dropout during training requires only O(n) computation per example per update\n",
    "- does not significantly limit the type of model or training procedure that can be used\n",
    "\n",
    "a large portion of the power of dropout arises from the fact that the masking noise is applied to the hidden units. (**adaptive destruction of the information of the input**)\n",
    "\n",
    "## Adversarial Training \n",
    "In many cases, neural networks have begun to reach human performance when evaluated on an i.i.d. test set. In order to probe... -> search for examples that the model misclassifies. \n",
    "\n",
    "Specifically, we search for $x^*$ that is very near data point $x$ st human observer cannot tell the difference but the network can make highly different prediction. -> **adversarial example** -> **adversarial training**\n",
    "![adversarial-example][adversarial-example]\n",
    "\n",
    "primary causes of these adversarial examples is **excessive linearity**. Linear functions are easy to optimize but its value can change very rapidly if it has numerous inputs\n",
    "\n",
    "## Tangent Distance, Tangent Prop, and Manifold Tangent Classifier\n",
    "Many machine learning algorithms aim to overcome the curse of dimensionality by assuming that the data lies near a **low-dimensional manifold**\n",
    "\n",
    "**tangent distance** algorithm: a non-parametric nearest-neighbor algorithm in which the metric used is not the generic Euclidean distance but one that is derived from knowledge of the manifolds near which probability concentrates\n",
    "\n",
    "->  we are trying to classify examples and that examples on the same manifold share the same category.\n",
    "\n",
    "**tangent prop** algorithm:\n",
    "Tangent propagation is closely related to dataset augmentation\n",
    "\n",
    "Resourses:\n",
    "- https://metacademy.org/graphs/concepts/weight_decay_neural_networks\n",
    "- https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a\n",
    "- https://deeplearning4j.org/earlystopping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[early-stopping]: early-stopping.png\n",
    "[representational-sparsity]: representational-sparsity.png\n",
    "[bagging]: bagging.png\n",
    "[adversarial-example]: adversarial-example.png\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
