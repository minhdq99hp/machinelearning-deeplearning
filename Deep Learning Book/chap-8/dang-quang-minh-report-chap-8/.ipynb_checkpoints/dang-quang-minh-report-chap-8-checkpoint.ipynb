{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 8: Optimization\n",
    "Of all of the many optimization problems involved in deep learning, the most difficult is neural network training\n",
    "\n",
    "This chapter focuses on one particular case of optimization: finding the parameters θ of a neural network that significantly reduce a cost function J(θ)\n",
    "\n",
    "## How Learning Differs from Pure Optimization\n",
    "We reduce a different cost function J(θ) in the hope that doing so will improve performance measure P -> minimizing J is the goal.\n",
    "\n",
    "Optimization algorithms for training deep models also typically include some specialization on the specific structure of machine learning objective functions.\n",
    "\n",
    "We often minimize the J where the expectation is taken across the data generating distribution $p_{data}$ rather than just over the finite training set:\n",
    "\n",
    "$J^∗(\\theta) = E_{(x,y)∼p_{data}} L(f(x; θ), y)$\n",
    "\n",
    "## Empirical Risk Minimization\n",
    "when we do not know $p_{data}(x, y)$ but only have a training set of samples, we have a machine learning problem.\n",
    "\n",
    "Solution: minimize **empirical risk** -> **empirical risk minimization**\n",
    "\n",
    "$J^∗(\\theta) = E_{(x,y)∼\\hat{p}_{data}(x,y)} L(f(x; θ), y) = \\frac{1}{m} \\sum_{i=1}^m L(f(x^{(1)}; θ), y^{(1)})$\n",
    "\n",
    "But, empirical risk minimization is prone to overfitting -> The most effective modern optimization\n",
    "algorithms are based on gradient descent.\n",
    "\n",
    "but many useful loss functions have no useful derivatives. -> we must use a slightly different approach, in which the quantity that we actually optimize is even more different from the quantity that we truly want to optimize.\n",
    "\n",
    "### Surrogate Loss Functions and Early Stopping\n",
    "Not every loss fuction can be optimized efficiently -> **surrogate loss function** such as the negative log-likelihood.\n",
    "\n",
    "A very important difference between optimization in general and optimization as we use it for training algorithms is that training algorithms do not usually halt at a local minimum.\n",
    "\n",
    "Ex: early stopping (halts when a convergence criterion is satified\n",
    "\n",
    "### Batch and Minibatch Algorithms\n",
    "One aspect of machine learning algorithms that separates them from general optimization algorithms is that the objective function usually decomposes as **a sum over the training examples**.\n",
    "\n",
    "randomly sampling a small number of examples from the dataset, then taking the average over only those examples. -> to reduce expensive computation.     \n",
    "\n",
    "Optimization algorithms that use the entire training set are called **batch** or **deterministic** gradient methods, because they process all of the training examples simultaneously in a large batch\n",
    "\n",
    "Optimization algorithms that use only a single example at a time are sometimes called **stochastic** or sometimes **online** methods.\n",
    "\n",
    "Most algo used for DL using more than one but less then all of the training examples. -> **minibatch** or **minibatch stochastic** methods\n",
    "\n",
    "#### Choosing minibatch size\n",
    "- Larger batches provide a more accurate estimate of the gradient\n",
    "- If all examples in the batch are to be processed in parallel then the amount of memory scales with the batch size\n",
    "- Some kinds of hardware achieve better runtime with specific sizes of arrays.\n",
    "- Small batches can offer a regularizing effect. Training with such a small batch size might require a small learning rate to maintain stability due to the high variance in the estimate of the gradient.\n",
    "\n",
    "Minibatches should be selected randomly.\n",
    "\n",
    "## Challenges in Neural Network Optimization\n",
    "When training neural networks, we must confront the general non-convex case\n",
    "\n",
    "### Ill-Conditioning\n",
    "Some challenges arise even when optimizing convex functions -> ill conditioning of the Hessian matrix H.\n",
    "\n",
    "Ill-conditioning can manifest by causing SGD to get “stuck” in the sense that even very small steps increase the cost function\n",
    "\n",
    "### Local Minima\n",
    "With non-convex functions, such as neural nets, it is possible to have many local minima (because of model identifiability issues).\n",
    "\n",
    "A test that can rule out local minima as the problem is to plot the norm of the gradient over time. If the norm of the gradient does not shrink to insignificant size, the problem is neither local minima nor any other kind of critical point.\n",
    "\n",
    "### Plateaus, Saddle Points and Other Flat Regions\n",
    "At a saddle point, the Hessian matrix has both positive and negative eigenvalues\n",
    "\n",
    "In low dimensional spaces, local minima are common. In higher dimensional spaces, local minima are rare and saddle points are more common\n",
    "\n",
    "Maxima: unmodified Newton’s method is attracted to them.\n",
    "Wide, flat regions: \n",
    "- In a convex problem, global minima,\n",
    "- In a general optimization problem, a high value of the objective function.\n",
    "\n",
    "### Cliffs and Exploding Gradients\n",
    "On the face of an extremely steep cliff structure, the gradient update step can move the parameters extremely far, usually jumping off of the cliff structure altogether\n",
    "![cliff][cliff]\n",
    "\n",
    "Solution: avoided using the **gradient clipping** \n",
    "\n",
    "### Long-Term Dependencies\n",
    "Another difficulty that neural network optimization algorithms must overcome arises when the computational graph becomes extremely deep.\n",
    "\n",
    "Vanishing gradients: make it difficult to know which direction the parameters should move to improve\n",
    "the cost function\n",
    "Exploding gradients: make learning unstable\n",
    "\n",
    "### Inexact Gradients\n",
    "Reasons: noisy or biased estimate, minibatch.\n",
    "\n",
    "Objective function is intractable -> gradient is intractable as well.\n",
    "\n",
    "### Poor Correspondence between Local and Global Structure\n",
    "Much of research into the difficulties of optimization has focused on whether training arrives at a global minimum, a local minimum, or a saddle point, but in practice neural networks do not arrive at a critical point of any kind.\n",
    "![poor-correspondence][poor-correspondence]\n",
    "\n",
    "\n",
    "## Basic Algorithms\n",
    "Gradient descent can be accelerate considerably by using stochastic gradient descent.\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "SGD and its variants are the most used optimization algo.\n",
    "\n",
    "follow the gradient of randomly selected minibatches downhill.\n",
    "\n",
    "it is necessary to gradually decrease the learning rate over time\n",
    "\n",
    "-> In practice, it is common to decay the learning rate linearly until iteration τ:\n",
    "\n",
    "$\\epsilon_k = (1-\\alpha) \\epsilon_0 + \\alpha \\epsilon_\\tau$\n",
    "\n",
    "After iteration τ, it is common to leave $\\epsilon$ constant.\n",
    "\n",
    "**Way to choose the learning rate**: monitoring the learning curve.\n",
    "\n",
    "**Setting the parameters**:\n",
    "- τ may be set to the number of iterations required to make a few hundred passes through the training set.\n",
    "- $\\epsilon_\\tau$ should be set to roughly $1\\%$ the value of $\\epsilon_0$\n",
    "- $\\epsilon_0$\n",
    "    - Too large: the learning curve will show violent oscillations\n",
    "    - Too low: learning will be slow, or may be it can stuck with a high cost value\n",
    "    \n",
    "Properties of SGD:\n",
    "- Computation time per update does not grow with the number of examples -> allow convergence even when the number of training data is very large.\n",
    "\n",
    "To study the convergence rate -> measure **excess error**: $J(\\theta) - \\min_\\theta (\\theta)$\n",
    "\n",
    "We should not pursue an optimization that converges faster than $\\mathcal{O}(\\frac{1}{k})$ to avoid overfitting.\n",
    "\n",
    "With large datasets, the ability of SGD to make rapid initial progress while evaluating the gradient for only very few examples outweighs its slow asymptotic convergence.\n",
    "\n",
    "### Momentum\n",
    "is designed to accelerate learning. by introduce the variable $v$ which play the role of velocity.\n",
    "\n",
    "-> SGD with momentum.\n",
    "\n",
    "**Physics perspective**:\n",
    "\n",
    "The particle experiences net force $f(t)$:\n",
    "- One force is proportional to the negative gradient of the cost function $−∇_θ J(θ)$\n",
    "- One force is proportional to $−v(t)$ (viscous drag) -> to make the particle lose its energy over time.\n",
    "\n",
    "### Nesterov Momentum\n",
    "The difference between Nesterov momentum and standard momentum is where the gradient is evaluated. With Nesterov momentum the gradient is valuated after the current velocity is applied.\n",
    "\n",
    "In batch gradient descent -> convergre with $\\mathcal{O}(1/k^2)$\n",
    "\n",
    "In stochastic gradient descent -> not improve at all.\n",
    "\n",
    "## Parameter Initialization Strategies\n",
    "Training algorithms for deep learning models are usually iterative in nature and thus require the user to specify some initial point from which to begin the iterations.\n",
    "\n",
    "-> Training deep models are strongly affected by the choice of initialization.\n",
    "\n",
    "The initial parameters need to “break symmetry” between different units.\n",
    "\n",
    "The goal of having each unit compute a different function motivates random initialization of the parameters. -> draw from Gauss or Uniform.\n",
    "\n",
    "Final parameters should be close to the initial parameters.\n",
    "\n",
    "-> draw from $\\mathcal{U}(-\\frac{1}{\\sqrt{m}}, \\frac{1}{\\sqrt{m}})$ or using **normalized initialization** $W_{i,j} ∼ \\mathcal{U}\\left(−\\sqrt{\\frac{6}{m+n}}, \\sqrt{\\frac{6}{m+n}}\\right)$\n",
    "\n",
    "In practice, we usually need to treat the scale of the weights as a hyperparameter whose optimal value lies somewhere roughly near but\n",
    "not exactly equal to the theoretical predictions.\n",
    "\n",
    "### Setting biases\n",
    "Setting the biases to zero is compatible with most weight initialization schemes. \n",
    "\n",
    "There are a few situations where we may set some biases to non-zero values:\n",
    "- a bias for output unit.\n",
    "- choose biases for  avoid causing too much saturation at initialization. \n",
    "- Sometimes a unit controls whether other units are able to participate in a function.\n",
    "    - Ex: forget gate of LSTM model.\n",
    "    \n",
    "**Choosing a variance or precision parameter**: We can usually initialize variance or precision parameters to 1 safely.\n",
    "\n",
    "initialize a supervised model with the parameters learned by an unsupervised model trained on the same inputs.\n",
    "\n",
    "## Algorithms with Adaptive Learning Rates\n",
    "Learning rate was reliably one of the hyperparameters that is the most difficult to set because it has a significant impact on model performance.\n",
    "\n",
    "The **delta-bar-delta** algo: early heuristic approach to adapting individual learning rates for model parameters during training.\n",
    "\n",
    "### AdaGrad\n",
    "Adapts the learning rates of all model parameters by: scaling them inversely proportional to the square\n",
    "root of the sum of all of their historical squared values.\n",
    "\n",
    "AdaGrad performs well for some but not all deep learning models. (because: the accumulation of squared gradients from the beginning of training can result in a premature and excessive decrease in the effective learning rate).\n",
    "\n",
    "-> good for convex optimization, not for non-convex\n",
    "\n",
    "### RMSProp\n",
    "Modified AdaGrad for non-convex: changing the gradient accumulation into an exponentially weighted moving average.\n",
    "\n",
    "-> good for training deep neural networks.\n",
    "\n",
    "### Adam\n",
    "\"adaptive moments.\" \n",
    "\n",
    "a variant on the combination of RMSProp and momentum\n",
    "\n",
    "### Choosing the Right Optimization Algorithm\n",
    "There is currently no consensus on this point.\n",
    "\n",
    "The most popular optimization algorithms actively in use include SGD, SGD with momentum, RMSProp, RMSProp with momentum, AdaDelta and Adam.\n",
    "\n",
    "## Approximate Second-Order Methods\n",
    "\n",
    "### Newton’s Method\n",
    "**second-order methods** make use of second derivatives to improve optimization.\n",
    "\n",
    "-> using a second-order Taylor series expansion to approximate $J(θ)$ near some point $θ_0$\n",
    "\n",
    "-> Newton parameter update rule: $θ^∗ = θ_0 − H^{−1} ∇_θ J(θ_0)$\n",
    "\n",
    "- a locally quadratic function (with positive definite H): Newton’s method jumps directly to the minimum (by rescaling the gradient by $H^{−1}$)\n",
    "- convex but not quadratic function: this update can be iterated\n",
    "\n",
    "In deep learning, the surface of the objective function is typically non-convex with many features, such as saddle points, that are problematic for Newton’s method \n",
    "\n",
    "-> regularizing the Hessian\n",
    "\n",
    "### Conjugate Gradients\n",
    "a method to efficiently avoid the calculation of the inverse Hessian by iteratively descending conjugate directions\n",
    "\n",
    "-> we seek to find a search direction that is conjugate to the previous line search direction\n",
    "\n",
    "At training iteration t, the next search direction $d_t$:\n",
    "\n",
    "$d_t = ∇_θ J(θ) + β_t d_{t−1}$\n",
    "\n",
    "$β_t$ is a coefficient whose magnitude controls how much of the direction. Two popular methods for computing the $β_t$.\n",
    "1. Fletcher-Reeves\n",
    "2. Polak-Ribière\n",
    "\n",
    "### BFGS\n",
    "BFGS is similar to the conjugate gradient method. \n",
    "\n",
    "BFGS takes a more direct approach to the approximation of Newton’s update.\n",
    "\n",
    "BFGS algorithm must store the inverse Hessian matrix, **M**, that requires $\\mathcal{O}(n^2)$ memory -> BFGS\n",
    "is impractical for most modern DL models.\n",
    "\n",
    "**Limited Memory BFGS (L-BFGS)**: avoiding storing the complete inverse Hessian approximation.\n",
    "\n",
    "## Optimization Strategies and Meta-Algorithms\n",
    "\n",
    "### Batch Normalization\n",
    "a method of adaptive reparametrization, motivated by the difficulty of training very deep models\n",
    "\n",
    "- Provides an elegant way of reparameterizing almost any network \n",
    "- Significantly reduces the problem of coordinating updates across many layers\n",
    "- Batch normalization can be applied to any input or hidden layer in a network\n",
    "\n",
    "$\\mathbf{H}$: a minibatch of activations of the layer to normalize\n",
    "\n",
    "To normalize $\\mathbf{H}$, we replace it with\n",
    "$\\mathbf{H'} = \\frac{\\mathbf{H} − \\mathbf{\\mu}}{\\mathbf{\\sigma}}$\n",
    "\n",
    "### Coordinate Descent\n",
    "it may be possible to solve an optimization problem quickly by breaking it into separate pieces\n",
    "\n",
    "**coordinate descent**: because we optimize one coordinate at a time\n",
    "**block coordinate descent**: minimizing with respect to a subset of the variables simultaneously.\n",
    "\n",
    "Coordinate descent makes the most sense when:\n",
    "- the different variables in the optimization problem can be clearly separated into groups that play relatively isolated roles\n",
    "- optimization with respect to one group of variables is significantly more efficient than optimization with respect to all of the variables.\n",
    "\n",
    "### Supervised Pretraining\n",
    "Directly training a model to solve a specific task can be too ambitious if the model is complex and hard to ptimize or if the task is very difficult.\n",
    "\n",
    "**Pretraining**: strategies that involve training simple models on simple tasks before confronting the challenge of\n",
    "training the desired model to perform the desired task.\n",
    "\n",
    "Pretraining, and especially greedy pretraining, algorithms are ubiquitous in deep learning\n",
    "\n",
    "**Greedy Supervised Pretraining**: break supervised learning problems into other simpler supervised learning problems\n",
    "\n",
    "### Designing Models to Aid Optimization\n",
    "Designing the models to be easier to optimize instead of improving the optimization algorithms.\n",
    "\n",
    "it is more important to choose a model family that is easy to optimize than to use a powerful optimization algorithm.\n",
    "\n",
    "### Continuation Methods and Curriculum Learning\n",
    "**Continuation methods**: strategies that can make optimization easier by choosing initial points to ensure that local optimization spends most of its time in well-behaved regions of space\n",
    "\n",
    "Traditional continuation methods are based on smoothing the objective function. \n",
    "\n",
    "Continuation methods traditionally were mostly designed with the goal of overcoming the challenge of local minima. -> reach global minimum despite the presence of many local minima.\n",
    "\n",
    "-> Blurring the original cost function by approximating: \n",
    "\n",
    "$J^{(i)}(θ) = E_{θ'∼\\mathcal{N} (θ';θ,σ^{(i)2})} J(θ')$\n",
    "\n",
    "An approach called **curriculum learning** or **shaping** can be interpreted as a continuation method.\n",
    "\n",
    "**Curriculum learning**: is based on the idea of planning a learning process to begin by learning simple concepts and progress to learning more complex concepts that depend on these simpler concepts.\n",
    "\n",
    "[cliff]: cliff.png\n",
    "[poor-correspondence]: poor-correspondence.png"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
