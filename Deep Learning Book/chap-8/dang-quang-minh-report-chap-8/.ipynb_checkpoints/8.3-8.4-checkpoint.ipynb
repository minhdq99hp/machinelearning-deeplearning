{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Algorithms\n",
    "Gradient descent can be accelerate considerably by using stochastic gradient descent.\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "SGD and its variants are the most used optimization algo.\n",
    "\n",
    "follow the gradient of randomly selected minibatches downhill.\n",
    "\n",
    "it is necessary to gradually decrease the learning rate over time\n",
    "\n",
    "-> In practice, it is common to decay the learning rate linearly until iteration τ:\n",
    "\n",
    "$\\epsilon_k = (1-\\alpha) \\epsilon_0 + \\alpha \\epsilon_\\tau$\n",
    "\n",
    "After iteration τ, it is common to leave $\\epsilon$ constant.\n",
    "\n",
    "**Way to choose the learning rate**: monitoring the learning curve.\n",
    "\n",
    "**Setting the parameters**:\n",
    "- τ may be set to the number of iterations required to make a few hundred passes through the training set.\n",
    "- $\\epsilon_\\tau$ should be set to roughly $1\\%$ the value of $\\epsilon_0$\n",
    "- $\\epsilon_0$\n",
    "    - Too large: the learning curve will show violent oscillations\n",
    "    - Too low: learning will be slow, or may be it can stuck with a high cost value\n",
    "    \n",
    "Properties of SGD:\n",
    "- Computation time per update does not grow with the number of examples -> allow convergence even when the number of training data is very large.\n",
    "\n",
    "To study the convergence rate -> measure **excess error**: $J(\\theta) - \\min_\\theta (\\theta)$\n",
    "\n",
    "We should not pursue an optimization that converges faster than $\\mathcal{O}(\\frac{1}{k})$ to avoid overfitting.\n",
    "\n",
    "With large datasets, the ability of SGD to make rapid initial progress while evaluating the gradient for only very few examples outweighs its slow asymptotic convergence.\n",
    "\n",
    "### Momentum\n",
    "is designed to accelerate learning. by introduce the variable $v$ which play the role of velocity.\n",
    "\n",
    "-> SGD with momentum.\n",
    "\n",
    "**Physics perspective**:\n",
    "\n",
    "The particle experiences net force $f(t)$:\n",
    "- One force is proportional to the negative gradient of the cost function $−∇_θ J(θ)$\n",
    "- One force is proportional to $−v(t)$ (viscous drag) -> to make the particle lose its energy over time.\n",
    "\n",
    "### Nesterov Momentum\n",
    "The difference between Nesterov momentum and standard momentum is where the gradient is evaluated. With Nesterov momentum the gradient is valuated after the current velocity is applied.\n",
    "\n",
    "In batch gradient descent -> convergre with $\\mathcal{O}(1/k^2)$\n",
    "\n",
    "In stochastic gradient descent -> not improve at all.\n",
    "\n",
    "## Parameter Initialization Strategies\n",
    "Training algorithms for deep learning models are usually iterative in nature and thus require the user to specify some initial point from which to begin the iterations.\n",
    "\n",
    "-> Training deep models are strongly affected by the choice of initialization.\n",
    "\n",
    "The initial parameters need to “break symmetry” between different units.\n",
    "\n",
    "The goal of having each unit compute a different function motivates random initialization of the parameters. -> draw from Gauss or Uniform.\n",
    "\n",
    "Final parameters should be close to the initial parameters.\n",
    "\n",
    "-> draw from $\\mathcal{U}(-\\frac{1}{\\sqrt{m}}, \\frac{1}{\\sqrt{m}})$ or using **normalized initialization** $W_{i,j} ∼ \\mathcal{U}\\left(−\\sqrt{\\frac{6}{m+n}}, \\sqrt{\\frac{6}{m+n}}\\right)$\n",
    "\n",
    "In practice, we usually need to treat the scale of the weights as a hyperparameter whose optimal value lies somewhere roughly near but\n",
    "not exactly equal to the theoretical predictions.\n",
    "\n",
    "### Setting biases\n",
    "Setting the biases to zero is compatible with most weight initialization schemes. \n",
    "\n",
    "There are a few situations where we may set some biases to non-zero values:\n",
    "- a bias for output unit.\n",
    "- choose biases for  avoid causing too much saturation at initialization. \n",
    "- Sometimes a unit controls whether other units are able to participate in a function.\n",
    "    - Ex: forget gate of LSTM model.\n",
    "    \n",
    "**Choosing a variance or precision parameter**: We can usually initialize variance or precision parameters to 1 safely.\n",
    "\n",
    "initialize a supervised model with the parameters learned by an unsupervised model trained on the same inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
